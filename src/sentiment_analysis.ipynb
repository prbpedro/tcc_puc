{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('tcc_puc': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "77642b1de94b3ecd25ac7cee94ae6d4cd14a15f998248e7a28e862a045f813e3"
   }
  },
  "interpreter": {
   "hash": "77642b1de94b3ecd25ac7cee94ae6d4cd14a15f998248e7a28e862a045f813e3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### This notebook should create a csv with the Elon Musk Tweets, containg btc releated info, sentimental analisys scores and the btc prices indexed by date"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/prbpedro/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "base_path_to_csv = os.path.join(os.getcwd() + '/eltweets/*.csv')\n",
    "csv_list = glob.glob(base_path_to_csv)\n",
    "\n",
    "df_list = [pd.read_csv(csv, index_col='id') for csv in csv_list]\n",
    "df = pd.concat(df_list)\n",
    "df = df.reset_index().drop_duplicates(subset='id', keep='first').set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing tweets to sentimental analisys\n",
    "\n",
    "df['full_text'] = df['full_text'].astype('unicode')\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ', ' ', x)\n",
    "remove_users_ref = lambda x: re.sub(\"@[A-Za-z0-9]+\",\"\",x)\n",
    "remove_links = lambda x: re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", x)\n",
    "remove_hashtags_underlines = lambda x: x.replace(\"#\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "df['full_text'] = df['full_text'].map(remove_rt).map(remove_users_ref)\n",
    "df['full_text'] = df['full_text'].map(remove_links)\n",
    "df['full_text'] = df['full_text'].map(remove_hashtags_underlines)\n",
    "\n",
    "df['full_text'] = df['full_text'].str.lower()\n",
    "df = df[(\n",
    "    df['full_text'].str.contains(\"bitcoin\") | \n",
    "    df['full_text'].str.contains(\"btc\") | \n",
    "    df['full_text'].str.contains(\"etherum\") | \n",
    "    df['full_text'].str.contains(\"eth \") | \n",
    "    df['full_text'].str.contains(\"crypto\") | \n",
    "    df['full_text'].str.contains(\"doge\"))]\n",
    "df['full_text'] = df['full_text'].str.replace('&amp;', 'and')\n",
    "df['full_text'] = df['full_text'].str.replace('&', 'and')\n",
    "df['full_text'] = df['full_text'].str.replace('ðŸ’”', 'broke my heart')\n",
    "df['full_text'] = df['full_text'].str.replace('ðŸ¤£', 'laughing ')\n",
    "df['full_text'] = df['full_text'].str.replace('ðŸŽ¶', '')\n",
    "df['full_text'] = df['full_text'].str.replace(\"itâ€™s\", 'it is')\n",
    "df['full_text'] = df['full_text'].str.replace(\"donâ€™t\", 'do not')\n",
    "df['full_text'] = df['full_text'].str.replace(\"canâ€™t\", 'can not')\n",
    "df['full_text'] = df['full_text'].str.replace(\"wonâ€™t\", 'will not')\n",
    "df['full_text'] = df['full_text'].str.replace(\"peopleâ€™s\", 'people')\n",
    "df['full_text'] = df['full_text'].str.replace(\"peopleâ€™s\", 'people')\n",
    "df['full_text'] = df['full_text'].str.replace(\"thereâ€™s\", 'there is')\n",
    " \n",
    "remove_pontuacao = lambda x:  re.sub(r'[^\\w\\s]', '', x)\n",
    "df['full_text'] = df['full_text'].map(remove_pontuacao)\n",
    "\n",
    "df['full_text'] = df['full_text'].str.replace(r'\\\\n',' ', regex=True) \n",
    "\n",
    "remove_multiplos_espacos = lambda x:  re.sub(' +', ' ', x)\n",
    "df['full_text'] = df['full_text'].map(remove_multiplos_espacos)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df['full_text'] = df['full_text'].astype('unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing created_at column \n",
    "# Creating influence_end_at column containing the date time that indicates the end of the influence of the tweet in the btc price\n",
    "\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "df['created_at'] = df['created_at'].dt.normalize()\n",
    "df['influence_end_at'] = df['created_at']  + pd.DateOffset(days=1)\n",
    "df['influence_end_at'] = df['influence_end_at'].dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating full_text_score column with the compound value of the sentimental analisys of each tweet\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "score_full_text = lambda x: analyzer.polarity_scores(x)['compound']\n",
    "\n",
    "df['full_text_score'] = None\n",
    "df['full_text_score'] = df['full_text'].map(score_full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicating rows that contains differents created_at and influence_end_at column values\n",
    "\n",
    "new_rows = df[df['created_at'] != df['influence_end_at']]\n",
    "new_rows['created_at'] = new_rows['influence_end_at']\n",
    "new_rows['id'] = None\n",
    "\n",
    "new_df = df.append(new_rows)\n",
    "\n",
    "mask = df['created_at'].duplicated(keep=False)\n",
    "duplicados = df[mask]\n",
    "\n",
    "f_df = df[~mask].copy()\n",
    "f_df['Date'] = f_df['created_at']\n",
    "f_df['Score'] = f_df['full_text_score']\n",
    "f_df = f_df[['Date', 'Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary with the tweet influence_end_at as key and the sum of the scores of sentimental analisys of tweets of that date and the number of tweets in that day as the value\n",
    "\n",
    "m = {}\n",
    "for d in duplicados['created_at'].unique():\n",
    "    if d not in m.keys():\n",
    "        m[d] = { 'count': 0, 'full_text_score': 0.0 }\n",
    "    for i, row in df.loc[df['created_at'] == d].iterrows():\n",
    "        m[d]['full_text_score'] += row['full_text_score']\n",
    "        m[d]['count'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating csv file with columns Date and Score (Arithmetic mean of the twwets sentimental analisys score of that date )\n",
    "\n",
    "f_df2 = pd.DataFrame([ {'Date': k, 'Score': v['full_text_score'] / v['count'] } for k, v in m.items()])\n",
    "\n",
    "df = pd.concat([f_df, f_df2], ignore_index=True)\n",
    "df.set_index('Date', inplace=True, drop=True)\n",
    "df = df[df['Score'] !=0]\n",
    "df['Score'] = df['Score'].round(decimals=4)\n",
    "df.to_csv(\"btc_em_sentimental_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and normalizing csv containing the btc historial prices\n",
    "\n",
    "btc_df = pd.read_csv(\"Bitcoin Historical Data - Investing.com.csv\", usecols = ['Date','Price'])\n",
    "btc_df['Date'] = pd.to_datetime(btc_df['Date'])\n",
    "btc_df['Date'] = btc_df['Date'].dt.normalize()\n",
    "btc_df = btc_df.reset_index().set_index('Date')\n",
    "btc_df = btc_df.drop(['index'], axis=1)\n",
    "btc_df['Price'] = btc_df['Price'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the btc historical price data with the sentimental analisys data\n",
    "\n",
    "df.index = df.index.tz_localize(None)\n",
    "btc_df.index = btc_df.index.tz_localize(None)\n",
    "merged = pd.merge(df, btc_df, on = ['Date'], how = 'outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating final csv file with Date, Score and Price columns\n",
    "\n",
    "merged['Score'] = merged['Score'].fillna(0)\n",
    "merged['Score'] = pd.to_numeric(merged['Score'])\n",
    "merged['Price'] = pd.to_numeric(merged['Price'])\n",
    "merged.to_csv('btc_value_em_tweets_sentimental_score.csv')"
   ]
  }
 ]
}